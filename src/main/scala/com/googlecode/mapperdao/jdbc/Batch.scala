package com.googlecode.mapperdao.jdbc

import org.springframework.jdbc.core.JdbcOperations
import org.springframework.jdbc.core.BatchPreparedStatementSetter
import org.springframework.jdbc.core.PreparedStatementCallback
import java.sql.PreparedStatement
import org.springframework.jdbc.core.InterruptibleBatchPreparedStatementSetter
import org.springframework.jdbc.support.JdbcUtils
import org.springframework.jdbc.core.ColumnMapRowMapper
import org.springframework.jdbc.core.PreparedStatementCreator
import java.sql.Connection
import java.sql.Statement
import scala.collection.mutable.ArrayBuffer
import java.sql.ResultSet
import org.springframework.jdbc.core.SqlParameterValue
import org.springframework.jdbc.core.StatementCreatorUtils
import org.springframework.jdbc.core.SqlTypeValue

/**
 * batch inserts/updates
 *
 * @author kostantinos.kougios
 *
 * Nov 14, 2012
 */
protected class Batch(jdbc: JdbcOperations) {
	def batchUpdate(sql: String, args: Array[Array[SqlParameterValue]], options: Batch.Options) = {

		val pss = new BatchPreparedStatementSetter {
			def setValues(ps: PreparedStatement, i: Int) = {
				var idx = 1
				args(i).foreach { arg =>
					StatementCreatorUtils.setParameterValue(ps, idx, SqlTypeValue.TYPE_UNKNOWN, arg)
					idx += 1
				}
			}
			def getBatchSize = args.length
		}

		jdbc.execute(new PreparedStatementCreator {
			override def createPreparedStatement(con: Connection) = options.prepareStatement(con, sql)
		}, new PreparedStatementCallback[BatchResult] {
			override def doInPreparedStatement(ps: PreparedStatement) = {
				val result = options.process(ps, pss)
				val keys = options.keys(ps, result.length)
				BatchResult(result, keys)
				//					List<Integer> rowsAffected = new ArrayList<Integer>();
				//					for (int i = 0; i < batchSize; i++)
				//					{
				//						pss.setValues(ps, i);
				//						if (ipss != null && ipss.isBatchExhausted(i))
				//						{
				//							break;
				//						}
				//						rowsAffected.add(ps.executeUpdate());
				//					}
				//					int[] rowsAffectedArray = new int[rowsAffected.size()];
				//					for (int i = 0; i < rowsAffectedArray.length; i++)
				//					{
				//						rowsAffectedArray[i] = rowsAffected.get(i);
				//					}
				//					return rowsAffectedArray;
			}
		})
	}
}
object Batch {
	private[mapperdao] trait Options {
		def prepareStatement(con: Connection, sql: String): PreparedStatement
		def process(ps: PreparedStatement, pss: BatchPreparedStatementSetter): Array[Int]
		def keys(ps: PreparedStatement, numOfKeys: Int): Array[java.util.Map[String, Object]]
	}

	private val rm = new ColumnMapRowMapper
	private def statementWithAutogenerated(con: Connection, sql: String) = con.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS)

	object BatchWithAutogenerated extends Options {
		def prepareStatement(con: Connection, sql: String) = statementWithAutogenerated(con, sql)
		def process(ps: PreparedStatement, pss: BatchPreparedStatementSetter) = {
			for (i <- 0 until pss.getBatchSize) {
				pss.setValues(ps, i)
				ps.addBatch()
			}
			ps.executeBatch
		}
		def keys(ps: PreparedStatement, numOfKeys: Int) = {
			var idx = 0
			val keys = new Array[java.util.Map[String, Object]](numOfKeys)
			val generatedKeys = ps.getGeneratedKeys
			try {
				while (generatedKeys.next) {
					keys(idx) = rm.mapRow(generatedKeys, idx)
					idx += 1
				}
				if (idx != numOfKeys) throw new IllegalStateException("expected " + numOfKeys + " autogenerated keys but was " + idx)
				keys
			} finally {
				generatedKeys.close()
			}
		}
	}
}
case class BatchResult(rowsAffected: Array[Int], keys: Array[java.util.Map[String, Object]])